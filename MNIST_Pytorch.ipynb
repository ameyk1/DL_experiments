{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed Precision on 5-layer CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Following [Webinar from Nvidia](https://info.nvidia.com/webinar-mixed-precision-with-pytorch-reg-page.html)\n",
    "- Import PyTorch and [NVIDIA AMP](https://github.com/NVIDIA/apex) Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from apex import amp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Usage\n",
    "Check If you are using either Volta or Turing Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_id_t = torch.cuda.current_device()\n",
    "device_name_t = torch.cuda.get_device_name(device_id_t)\n",
    "print(device_name_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 256\n",
    "learning_rate = 0.001\n",
    "ite = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download dataset from PyTorch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='MNISTdata/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='MNISTdata/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloading with Batch Size\n",
    "- Check how many num_workers you should use using: htop , understand number of CPUs and its usage\n",
    "- Check how much of GPU you are using during training using : nvidia-smi -lms\n",
    "- Add pin_memory=True for fast data loading during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True, num_workers=16, pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer Size Calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_of_neural_net(in_channels, kernel_size, stride, padding):\n",
    "    out_width = (in_channels-kernel_size+2*padding)\n",
    "    out_width = out_width/stride\n",
    "    return int(out_width+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = len(train_dataset.train_data[0][0])\n",
    "channel_layer = [16,32,64]\n",
    "KERNEL_SIZE_Conv = 5\n",
    "STRIDE_SIZE_Conv = 1\n",
    "PADDING_SIZE_Conv = 2\n",
    "\n",
    "KERNEL_SIZE_pool = 2\n",
    "STRIDE_SIZE_pool = 2\n",
    "PADDING_SIZE_pool = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Network Class and Check How many layers are essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10, ite=0):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv = torch.nn.Sequential()\n",
    "        self.conv.add_module(\"conv_1\", nn.Conv2d(1, channel_layer[0], kernel_size=KERNEL_SIZE_Conv, stride=STRIDE_SIZE_Conv, padding=PADDING_SIZE_Conv))\n",
    "        layer_size_00 = size_of_neural_net(image_size,KERNEL_SIZE_Conv, STRIDE_SIZE_Conv, PADDING_SIZE_Conv)\n",
    "        self.conv.add_module(\"relu_1\", nn.ReLU())\n",
    "        self.conv.add_module(\"maxpool_1\", nn.MaxPool2d(kernel_size=KERNEL_SIZE_pool, stride=STRIDE_SIZE_pool))\n",
    "        layer_size_01 = size_of_neural_net(layer_size_00,KERNEL_SIZE_pool, STRIDE_SIZE_pool, PADDING_SIZE_pool)\n",
    "        lin_input = layer_size_01*layer_size_01*channel_layer[0]\n",
    "        if (ite > 0):\n",
    "            self.conv.add_module(\"conv_2\", nn.Conv2d(channel_layer[0], channel_layer[1], kernel_size=KERNEL_SIZE_Conv, stride=STRIDE_SIZE_Conv, padding=PADDING_SIZE_Conv))\n",
    "            layer_size_10 = size_of_neural_net(layer_size_01,KERNEL_SIZE_Conv, STRIDE_SIZE_Conv, PADDING_SIZE_Conv)\n",
    "            self.conv.add_module(\"relu_2\", nn.ReLU())\n",
    "            self.conv.add_module(\"maxpool_2\", nn.MaxPool2d(kernel_size=KERNEL_SIZE_pool, stride=STRIDE_SIZE_pool))\n",
    "            layer_size_11 = size_of_neural_net(layer_size_10,KERNEL_SIZE_pool, STRIDE_SIZE_pool, PADDING_SIZE_pool)\n",
    "            lin_input = layer_size_11*layer_size_11*channel_layer[1]\n",
    "        if (ite > 1):\n",
    "            self.conv.add_module(\"conv_3\", nn.Conv2d(channel_layer[1], channel_layer[2], kernel_size=KERNEL_SIZE_Conv, stride=STRIDE_SIZE_Conv, padding=PADDING_SIZE_Conv))\n",
    "            layer_size_20 = size_of_neural_net(layer_size_11,KERNEL_SIZE_Conv, STRIDE_SIZE_Conv, PADDING_SIZE_Conv)\n",
    "            self.conv.add_module(\"relu_3\", nn.ReLU())\n",
    "            self.conv.add_module(\"maxpool_3\", nn.MaxPool2d(kernel_size=KERNEL_SIZE_pool, stride=STRIDE_SIZE_pool))\n",
    "            layer_size_21 = size_of_neural_net(layer_size_20,KERNEL_SIZE_pool, STRIDE_SIZE_pool, PADDING_SIZE_pool)\n",
    "            lin_input = layer_size_21*layer_size_21*channel_layer[2]\n",
    "            \n",
    "        self.fc = torch.nn.Sequential()\n",
    "        self.fc.add_module(\"fc1\", torch.nn.Linear(lin_input, 512))\n",
    "        self.fc.add_module(\"fc2\", torch.nn.Linear(512, num_classes))\n",
    "    def forward(self, x):       \n",
    "        out = self.conv(x)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "total_step = len(train_loader)\n",
    "for i in range(3):\n",
    "    model = ConvNet(num_classes,i).to(device)\n",
    "    print(model)\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model, optimizer = amp.initialize (model, optimizer, opt_level=\"O1\")\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "    \n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
